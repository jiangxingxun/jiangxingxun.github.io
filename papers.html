<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Xingxun Jiang's Publication</title>
	<meta content="Xingxun Jiang's Publication, jiangxingxun.github.io/papers.html" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight:bold;
}

ul { 
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight: bold;
  color: #FF0000;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.paper div {
  padding-left: 230px;
}

img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45959174-3', 'wangzheallen.github.io');
  ga('send', 'pageview');

</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');

</script>

<body>




<!-- <strong>Conference Reviewer</strong> -->

<div class="spanner"></div>
</div>
</div>
</div>



<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Xingxun Jiang's Publications</h2>
<h5>Note: Electronic papers shared on this website are for fast dissemination of research work. The copyrights of the papers belong to the corresponding publishers.</h5>	

<!--
<div class="paper">
<ul>
<li>Year 2023</li>
<li><u>Haolin Jiang</u>, Wenming Zheng, Yuan Zong, Xiaolin Xu, <u>Xingun Jiang</u>, Yunlong Xue, 
	“Geometric magnification-based Attention Graph Convolutional Network for Skeleton-based Micro-Gesture Recognition”, 
	<strong>2023 IEEE International Conference on Image Processing (ICIP 2023)</strong>, 2023. 
	[<a href='papers/xx.pdf'>Paper</a>]</li>
</ul>
</div>
 -->

	
<div class="paper">
<ul>
<li><u>Xingxun Jiang</u>, Yuan Zong, Wenming Zheng, Chuangao Tang, Wanchuang Xia, Cheng Lu, Jiateng Liu, 
	“DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions in the Wild”, 
	<strong>ACM 28th Conference on Multimedia (ACM MM2020)</strong>, pp.2881-2889, 2020. 
	[<a href='papers/DFEW/jiang_DFEW.pdf'>Paper</a>/<a href="papers/DFEW/jiang_DFEW_CN.pdf">中文版</a>]</li>
<li><u>Xingxun Jiang</u>, Yuan Zong, Wenming Zheng, Jiateng Liu, Mengting Wei,
	“Seeking Salient Facial Regions for Cross-Database Micro-Expression Recognition”,
	<strong>The 26th International Conference on Pattern Recognition (ICPR2022)</strong>, 2022.
	[<a href="papers/TGSR/jiang_TGSR.pdf">pdf</a>]</li>
<li>Mengting Wei,<u>Xingxun Jiang</u>, Wenming Zheng, Yuan Zong, Cheng Lu, Jiateng Liu, 
	“CMNet: Contrastive Magnification Network for Micro-Expression Recognition”,
	<strong>Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI2023)</strong>, 2023.	
	[<a href="papers/weimengting_cmnet.pdf">pdf</a>]</li>
<li>Xilei Zhang, <u>Xingxun Jiang</u>, Xiangyong Yuan, Wenming Zheng, 
	“Attentional focus modulates automatic finger-tapping movements”, 
	<strong>Scientific Reports</strong>, Vol.11, No.1, pp.1-13, 2021.
	[<a href='papers/zhang_AFM/zhang_AFM.pdf'>Paper</a>][<a href="https://github.com/jiangxingxun/AFM">Code</a>]</li>
<li>Haolin Jiang, Wenming Zheng, Yuan Zong, Xiaolin Xu, <u>Xingun Jiang</u>, Yunlong Xue, 
	“Geometric Magnification-based Attention Graph Convolutional Network for Skeleton-based Micro-Gesture Recognition”, 
	<strong>2023 IEEE International Conference on Image Processing (ICIP 2023)</strong>, 2023. 
	[<a href='papers/xx.pdf'>Paper</a>]</li>
<li>Mengting Wei, Wenming Zheng, <u>Xingxun Jiang</u>, Yuan Zong, Cheng Lu, Jiateng Liu,
	“A Novel Magnification-Robust Network with Sparse Self-Attention for Micro-expression Recognition”,
	<strong>The 26th International Conference on Pattern Recognition (ICPR2022)</strong>, 2022.
	[<a href="papers/weimengting_ssa.pdf">pdf</a>]</li>
<li>Mengting Wei, Wenming Zheng, Yuan Zong, <u>Xingxun Jiang</u>, Cheng Lu, Jiateng Liu,
	“A Novel Micro-expression Recognition Approach Using Attention-based Magnification-Adaptive Networks”, 
	<strong>2022 International Conference on Acoustics, Speech and Signal Processing (ICASSP2022)</strong>, 2022.
	[<a href="papers/weimengting_aman.pdf">pdf</a>]</li>
<li>Xiaolin Xu, Wenming Zheng, Yuan Zong, Cheng Lu, <u>Xingxun Jiang</u>,
	“Sample Self-Revised Network for Cross-Dataset Facial Expression Recognition”, 
	<strong>2022 International Joint Conference on Neural Networks (IJCNN2022)</strong>, 2022.
	[<a href="papers/xuxiaolin_ssrn.pdf">pdf</a>]</li>
<li>Yunlong Xue, Wenming Zheng, Yuan Zong, Hongli Chang, <u>Xingxun Jiang</u>,
	“Adaptive Hierarchical Graph Convolutional Network for EEG Emotion Recognition”,
	<strong>2022 International Joint Conference on Neural Networks (IJCNN2022)</strong>, 2022.
	[<a href="papers/xueyunlong_ahgcn.pdf">pdf</a>]</li>
<li>Sunan Li, Wenming Zheng, Yuan Zong, Cheng Lu, Chuangao Tang, <u>Xingxun Jiang</u>, Jiateng Liu, Wanchuang Xia, 
	“Bi-modality Fusion for Emotion Recognition in the Wild”, 
	<strong>The 21th ACM International Conference on Multimodal Interaction (ICMI2019)</strong>, pp.589-594, 2019. (竞赛冠军)
	[<a href='contests/EmotiW2019/EmotiW2019.pdf'>Paper</a>]</li>
<li>Mengting Wei, Yuan Zong, <u>Xingxun Jiang</u>, Cheng Lu, Jiateng Liu,
	“Micro-Expression Recognition Using Uncertainty-Aware Magnification-Robust Networks”,
	<strong>Entropy</strong>,2022.
	[<a href="papers/weimengting_uamrn.pdf">pdf</a>]</li>
<li>Xiaolin Xu, Yuan Zong, Cheng Lu, <u>Xingxun Jiang</u>,
	“Enhanced Sample Self-Revised Network for Cross-Dataset Facial Expression Recognition”,
	<strong>Entropy</strong>,2022.
	[<a href="papers/xuxiaolin_essrn.pdf">pdf</a>]</li>
<li>Wanchuang Xia, Wenming Zheng, Yuan Zong, <u>Xingxun Jiang</u>,
	“Motion Attention Deep Transfer Network for Cross-Database Micro-Expression Recognition”,
	<strong>ICPR workshop on Facial and Body Expressions, micro-expressions and behavior recognition (FBE2020)</strong>, 2020.
	[<a href="papers/xia_MADTN/xia_MADTN.pdf">Paper</a>/<a href="papers/xia_MADTN/xia_MADTN_CN.pdf">中文版</a>]</li>	
	
	
</ul>
<div class="spanner"></div>
</div>
</div>
	





<hr>
</body>
</html>
