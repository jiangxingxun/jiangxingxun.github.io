<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Xingxun Jiang</title>
	<meta content="Xingxun Jiang, jiangxingxun.github.io" name="keywords" />
	<style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #1772d0;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 800px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #eee;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight:bold;
}

ul { 
  list-style: circle;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 13px;
  font-weight: bold;
  color: #FF0000;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}

div.paper div {
  padding-left: 230px;
}

img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}

div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45959174-3', 'wangzheallen.github.io');
  ga('send', 'pageview');

</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');

</script>
<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
<div style="margin: 0px auto; width: 100%;">
<img title="Xingxun Jiang" style="float: left; padding-left: .01em; height: 140px;" src="./jiangxingxun.jpg" />
<div style="padding-left: 10em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Xingxun Jiang(江星洵)</span><br />
<span><strong>Ph.D. Candidate</strong></span><br />
<span><a href="http://aip.seu.edu.cn/">Affective Information Processing Lab,</a></span><br />
<span><a href="https://bme.seu.edu.cn/">School of Biological Sciences and Medical Engineering,</a></span><br />
<span><a href="https://www.seu.edu.cn/">Southeast University.</a></span><br />

<span><strong>Office </strong>: Room 318 (Middle), Liwenzheng Building</span> <br /> 
<span><strong>Email  </strong>: jiangxingxun [at] seu.edu.cn; jiangxingxun [at] gmail.com</span> <br /> 
</div>
</div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
<div class="section">
<h2>About Me (<a href="jxx_cv.pdf">CV</a>)(<a href="jxx_resume.html">resume</a>)</h2>
<div class="paper">

Since March 2021, I have becomed a Ph.D. Candidate (Successive Master-Doctor Program) of 
<a href="http://aip.seu.edu.cn/">Affective Information Processing Lab(AIPL)</a>
 at 
<a href="https://bme.seu.edu.cn/">School of Biological Sciences and Medical Engineering</a> 
 and also affiliated with 
<a href="https://bme.seu.edu.cn/cdls/">Key Laboratory of Child Development and Learning Science of Ministry of Education</a>,
<a href="https://www.seu.edu.cn/">Southeast University</a>,
 advisored by
<a href="http://aip.seu.edu.cn/wp-admin/wmzheng/">Prof. Wenming Zheng(IET Fellow)</a> .
From 2022 to 2024, I was funded by 
 <a href="http://www.csc.edu.cn/">China Scholarship Council (CSC)</a>
 and working as a Joint Ph.D. student in 
 <a href="https://gyzhao-nm.github.io/Guoying">Prof. Guoying Zhao(IEEE/IAPR Fellow)’s group</a> 
 at the 
 <a href="https://www.oulu.fi/en/university/faculties-and-units/faculty-information-technology-and-electrical-engineering/center-machine-vision-and-signal-analysis">Center for Machine Vision and Signal Analysis (CMVS)</a>, 
 <a href="https://www.oulu.fi/en">University of Oulu</a>, 
 Finland. 	
<br /><br /> 

Previously, I becomed a master student under the supervision of 
<a href="http://aip.seu.edu.cn/wp-admin/wmzheng/">Prof. Wenming Zheng</a> in September 2018.
I received my B.S. degree in Smart Grid Information Engineering from <a href="http://www.njupt.edu.cn/">Nanjing University of Posts and Telecommunications</a> in June 2017, under the supervision of <a href="http://jszy.hhu.edu.cn/wyj101/">Dr. Yingjun Wu</a>. <br />

</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>News</h2>
  <div class="paper">
    <ul>
      <li> 2022-11-19: One paper is accepted by <a href="https://aaai.org/Conferences/AAAI-23/">AAAI</a>.</li>
      <li> 2022-07-12: Won the Scholarship Supported by <a href="http://www.csc.edu.cn/">China Scholarship Council</a>. </li>
      <li> 2022-05-18: Two papers are accepted by <a href="https://www.icpr2022.com/">ICPR</a>.</li>
      <li> 2022-05-02: Two papers are accepted by <a href="https://wcci2022.org/">IJCNN</a>.</li>
      <li> 2022-01-22: One paper is accepted by <a href="https://2022.ieeeicassp.org/">ICASSP</a>.</li>
      <li> 2020-12-17: One paper is accepted by <a href="https://www.nature.com/srep/">Scientific Reports</a>.</li>
      <li> 2020-11-11: One paper is accepted by <a href="https://www.micc.unifi.it/icpr2020.php"> ICPR </a> workshop on Facial and Body Expression, micro-expressions and behavior recognition (FBE2020).</li>
    	<li> <alert>2020-07-29: One paper is accepted by <a href="https://2020.acmmm.org/">ACM MM</a>.</alert></li>
      <li> 2020-06-28: Won the Chien-Shiung Wu · BME Scholarship, Southeast University.</li>
      <li> 2019-10-14: One paper is accepted by <a href="http://icmi.acm.org/2019/index.php?id=award">ICMI</a>. </li>
      <li> <alert>2019-06-27: Won the Champion of Audio-Video based Emotion Recognition Challenge of the 7th EmotiW Challenge.</alert> </li>
      <li> 2018-11-14: Won the Third Prize in the 15th National Post-Graduate Mathematical Contest in Modeling. </li>
      <li> 2018-10-24: Won the First Academic Scholarship, Southeast University. </li>
    </ul>
  </div>
</div>
</div>



<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Selected Publications[<a href="papers.html">More</a>]</h2>

<div class="paper" id="acmmm2020"><img class="paper" src="papers/DFEW/jiang_DFEW.png" title="DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions in the Wild" />
<div> <strong>DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions in the Wild</strong><br />
  Xingxun Jiang, Yuan Zong, Wenming Zheng, Chuangao Tang, Wanchuang Xia, Cheng Lu, Jiateng Liu <br />
 in ACM 28th Conference on Multimedia (ACM MM2020) <br />
[<a href='papers/DFEW/jiang_DFEW.pdf'>Paper</a>/<a href="papers/DFEW/jiang_DFEW_CN.pdf">中文版</a>]
[<a href="papers/DFEW/jiang_DFEW.bib">Bibtex</a>]
[<a href="https://dfew-dataset.github.io/">Download DFEW</a>]
[<a href="https://github.com/jiangxingxun/DFEW">Code</a>]
[<a href="papers/DFEW/jiang_DFEW_ppt.pdf">PPT</a>]
[<a href="papers/DFEW/jiang_DFEW_poster.pdf">poster</a>]
[<a href="https://dl.acm.org/doi/10.1145/3394171.3413620">video</a>]
[<a href="https://www.overleaf.com/read/bmkjwknscskc">Overleaf</a>/<a href="https://www.overleaf.com/read/cgzhxwwtstkz">中文版</a>]

<br />
<alert>An new large-scale ‘in-the-wild’ dynamic facial expression database, consisting of 16372 clips from thousands of movies, which annotated by 7 dimensional emotion distribution.</alert><br />  
</div>
<div class="spanner"></div>
</div>


	
<div class="paper" id="jiang_TGSR"><img class="paper" src="papers/TGSR/jiang_TGSR.png" title="Seeking Salient Facial Regions for Cross-Database Micro-Expression Recognition" />
  <div> <strong>Seeking Salient Facial Regions for Cross-Database Micro-Expression Recognition</strong><br />
Xingxun Jiang, Yuan Zong, Wenming Zheng, Jiateng Liu, Mengting Wei<br />
in The 26th International Conference on Pattern Recognition (ICPR), 2022.<br />
[<a href="papers/TGSR/jiang_TGSR.pdf">Paper</a>/<a href="papers/TGSR/jiang_TGSR_CN.pdf">中文版</a>]
[<a href="papers/TGSR/jiang_TGSR.bib">Bibtex</a>]
[<a href="papers/TGSR/jiang_TGSR_formula.pdf">推导过程</a>]
[<a href="https://github.com/jiangxingxun/Seeking_Salient_Facial_Region">Code</a>]
[<a href="papers/TGSR/jiang_TGSR_ppt.pdf">PPT</a>]
[<a href="papers/TGSR/jiang_TGSR_poster.pdf">poster</a>]
	  
<br />
  </div>
  <div class="spanner"></div>
  </div>

	



<div class="paper" id="icmi2019"><img class="paper" src="contests/EmotiW2019/EmotiW2019.jpg" title="Bi-modality Fusion for Emotion Recognition in the Wild" />
  <div> <strong>Bi-modality Fusion for Emotion Recognition in the Wild</strong><br />
    Sunan Li, Wenming Zheng, Yuan Zong, Cheng Lu, Chuangao Tang, Xingxun Jiang, Jiateng Liu, Wanchuang Xia <br />
  in The 21th ACM International Conference on Multimodal Interaction (ACM ICMI),2019. <br />
  [<a href='contests/EmotiW2019/EmotiW2019.pdf'>Paper</a>] 
  [<a href="contests/EmotiW2019/EmotiW2019.bib">Bibtex</a>]
  [<a href="contests/EmotiW2019/EmotiW2019_certificate.pdf">EmotiW2019 Champion</a>]  <br />   
  <alert>Obtain The Champion of the Audio-Video based Emotion Recognition Challenge of the 7th EmotiW Challenge(2019)</alert><br />
  </div>
  <div class="spanner"></div>
  </div>


<div class="paper" id="zhang_AFM"><img class="paper" src="papers/zhang_AFM/zhang_AFM.jpg" title="Attentional focus modulates automatic fnger‑tapping movements" />
<div> <strong>Attentional Focus Modulates Automatic Finger‑tapping Movements</strong><br />
Xilei Zhang, Xingxun Jiang, Xiangyong Yuan, Wenming Zheng <br />
in Scientific Reports, 2021 <br />
[<a href='papers/zhang_AFM/zhang_AFM.pdf'>Paper</a>]
[<a href="papers/zhang_AFM/zhang_AFM.bib">Bibtex</a>]   
[<a href="https://github.com/jiangxingxun/AFM">Code</a>]<br /><br />
<alert>These findings demonstrate compelling evidence that attention can modulate automatic movements and provide an empirical foundation for theories based on such modulation in controlling human behavior. </alert>
</div>
<div class="spanner"></div>
</div>

<!--
<div class="paper" id="xia_MADTN"><img class="paper" src="papers/xia_MADTN/xia_MADTN.png" title="Motion Attention Deep Transfer Network for Cross-Database Micro-Expression Recognition" />
  <div> <strong>Motion Attention Deep Transfer Network for Cross-Database Micro-Expression Recognition</strong><br />
Wanchuang Xia, Wenming Zheng, Yuan Zong, Xingxun Jiang<br />
in ICPR workshop on Facial and Body Expressions, micro-expressions and behavior recognition (FBE2020)<br />
[<a href="papers/xia_MADTN/xia_MADTN.pdf">Paper</a>/<a href="papers/xia_MADTN/xia_MADTN_CN.pdf">中文版</a>]
[<a href="papers/xia_MADTN/xia_MADTN.bib">Bibtex</a>]  <br /> 
  </div>
  <div class="spanner"></div>
  </div>
-->


</div>
</div>






<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Patents</h2>
<div class="paper">
<ul> 
<li>郑文明，<ins>江星洵</ins>，宗源，夏万闯，<strong>基于人脸局部区域特征学习的跨数据库微表情识别方法及装置 ZL 2019 1 0706550.8 <a href='patents/20190801.pdf'>[已授权]</a></strong></li>
<li>郑文明，夏万闯，宗源，<ins>江星洵</ins>，路成，刘佳腾，<strong>基于光流注意力神经网络的跨库微表情识别方法及装置 ZL 2019 1 0756936.X <a href='patents/20190816.pdf'>[已授权]</a></strong></li>
<li>郑文明，李阳，<ins>江星洵</ins>，宗源，<strong>一种基于双半球差异性模型的脑电情感识别方法及装置 201911343069.3 <a href='patents/20191224.pdf'>[已授权]</strong></a></li>
<li>宗源，江林，张佳成，郑文明，<ins>江星洵</ins>，刘佳腾，<strong>基于联合分布最小二乘回归的跨数据库语音情感识别方法及装置 202010372728.2 <a href="patents/20200506.pdf">[已授权]</a> </strong></li>
<li>宗源，<ins>江星洵</ins>，郑文明，李阳，路成，唐传高，李溯南，<strong>基于域选择迁移回归的跨数据库微表情识别方法及装置 202010030236.5 <a href='patents/20200113a.pdf'>[已授权]</a></strong></li>
<!-- <li>郑文明，<ins>江星洵</ins>，宗源，夏万闯，<strong>基于EC-STFL损失函数的自然场景动态表情识别方法及装置 202010831485.4 <a href="patents/20200818.pdf">[审中|实申]</a></strong></li> -->
<li>郑文明，李阳，<ins>江星洵</ins>，宗源，李溯南，<strong>基于可迁移注意力神经网络的脑电情感识别方法及装置 202010030240.1 <a href='patents/20200113b.pdf'>[审中|实申]</a></strong></li>
<li>郑文明，常洪丽，宗源，<ins>江星洵</ins>，唐传高，<strong>基于空时特征融合模型的跨领域脑电情感识别方法及装置 202111560169.9 <a href='patents/20211220.pdf'>[审中|实申]</a></strong></li>
<li>郑文明，魏梦婷，<ins>江星洵</ins>，宗源，常洪丽，<strong>基于稀疏化自注意力机制的微表情识别方法及装置 202210196086.4 <a href='patents/20220302.pdf'>[审中|实申]</a></strong></li> 
<li>郑文明，魏梦婷，<ins>江星洵</ins>，宗源，路成，<strong>样本自适应的微表情放大方法及装置 202211562229.5 <a href="patents/20221207.pdf">[审中|实申]</a></strong></li>
<li>郑文明，魏梦婷，宗源，<ins>江星洵</ins>，刘佳腾，薛云龙，<strong>基于对比放大网络的微表情识别方法及装置 202210605395.2 <a href='patents/20220531.pdf'>[审中|实申]</a></strong></li>
	
 
</ul>
<div class="spanner"></div>
</div>
</div>
</div>
	
	
	
<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Software Copyright</h2>
<div class="paper">
<ul> 
	<li>郑文明、<ins>江星洵</ins>、宗源、夏万闯、刘佳腾、唐传高、张佳成，<strong>AIPL动态表情标注软件，2021SR0273805 <a href='softwareCopyright/2021SR0273805.jpg'>[已登记]</a></strong></li>
	<li>郑文明、夏万闯、宗源、<ins>江星洵</ins>、唐传高、刘佳腾、张佳成，<strong>多人表情识别演示系统，2021SR0273738 <a href='softwareCopyright/2021SR0273738.jpg'>[已登记]</a></strong></li>
	<li>郑文明、唐传高、宗源、<ins>江星洵</ins>、刘佳腾，<strong>高危孤独症谱系障碍自动筛查系统，2021SR0273737 <a href='softwareCopyright/2021SR0273737.jpg'>[已登记]</a></strong></li>
	<li>郑文明、刘佳腾、宗源、张佳成、<ins>江星洵</ins>、夏万闯、唐传高，<strong>基于人体面部微表情的孤独症分析系统，2021SR0272958 <a href='softwareCopyright/2021SR0272958.jpg'>[已登记]</a></strong></li>
	<li>郑文明、张佳成、宗源、<ins>江星洵</ins>、刘佳腾，<strong>基于生理信号及面部表情的孤独症分析系统，2021SR0273739 <a href='softwareCopyright/2021SR0273739.jpg'>[已登记]</a></strong></li>
</ul>
<div class="spanner"></div>
</div>
</div>
</div>
	

	

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Honors and Awards</h2>
<div class="paper">
<ul>
<li><strong>The First Academic Scholarship(一等学业奖), Southeast University, 2022.</strong></li>
<li><strong> <a href="contests/jiang_ThreeGood2022.pdf">Three-good Graduate Student of Southeast University(东南大学三好研究生)</a>, Southeast University, 2022</strong></li>
<li><strong> Scholarship Awardee Selected by China Scholarship Council, 2022</strong></li>
<li><strong> <a href="contests/jiang_ExcellentVolunteer.pdf">Excellent Volunteer(优秀志愿者)</a>, Southeast University, 2021 </strong></li>
<li><strong> <a href="contests/jiang_ThreeGood2021.pdf">Three-good Graduate Student of Southeast University(东南大学三好研究生)</a>, Southeast University, 2021</li>
<li><strong> <a href="contests/ChienShiungWu_BME2020/jiang_ChienShiungWu_BME2020.jpeg">Chien-Shiung Wu·BME Scholarship(吴健雄·生医奖学金)</a>, Southeast University, 2020.</strong></li>
<li><strong><alert>The 7th EmotiW Challenge, 2019</alert></strong>:The Audio-Video based Emotion Recognition Challenge of, <strong>Rank: <a href="contests/EmotiW2019/EmotiW2019_certificate.pdf">Champion</a>.  </strong></li> 
<li><strong>The 15th National Post-Graduate Mathematical Contest in Modeling(华为杯数学建模竞赛), 2018, Rank:  <a href="contests/NPGMCM2018/NPGMCM2018_certificate.jpg">Third Prize</a>.  </strong></li> 
<li><strong>The First Academic Scholarship(一等学业奖), Southeast University, 2018.</strong></li>
<li><strong>The Mathematical Contest in Modeling/The Interdisciplinary Contest in Modeling(MCM/ICM) (美国大学生数学建模竞赛), 2015, Rank:  <a href="contests/MCM2015/MCM2015_certificate.pdf">Honorable Mention</a>. </strong></li>
</ul>
<div class="spanner"></div>
</div>
</div>




<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Academic Service</h2>
<div class="paper">
Reviewer of IEEE Transaction on Multimedia, Pattern Recognition, Expert Systems With Applications, Image and Vision Computing<br/ >
Reviewer of ACM Multimedia (MM), 2023<br/ >
Reviewer of ICASSP, 2024<br/ >
Reviewer of International Conference on Pattern Recognition (ICPR), 2022<br/ >
	
<div class="spanner"></div>
</div>
</div>
</div>


	
<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Teaching Assistant</h2>
<div class="paper">
Affective Computing and Artificial Intelligence, Southeast University, 2021<br/ >
Affective Computing, University of Oulu, 2023<br/ >
	
<div class="spanner"></div>
</div>
</div>
</div>


<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Learning Notes</h2>
<div class="paper">
<ul>
<li><strong>SCI论文写作笔记</strong> <a href="technical_reports/aimi_edit_note.pdf">[手写版]</a><a href="technical_reports/aimi_edit_pdf.pdf">[pdf版]</a></li>
<li><strong>图卷积笔记</strong><a href="technical_reports/graphConv_note.pdf">[手写版]</a></li>
</ul>
<div class="spanner"></div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<a href="ee.html"><h2 id="confpapers">Electrical Engineering</h2></a>
</div>
</div>

<div style="clear: both;">
<div class="section">
<a href="recommended.html"><h2 id="confpapers">Recommended</h2></a>
</div>
</div>

<div style="clear: both;">
<div class="section"><h2>Correspondence</h2>
<div class="paper">
Room 318 (Middle), Liwenzheng Building, Southeast University, Sipailou 2#, Nanjing, Jiangsu Province, 210096 P. R. China.
</div>
</div>
</div>

<div style="clear:both;">
<p align="right"><font size="5">Last Updated on 6th Oct., 2022</a></font></p>
<p align="right"><font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font></p>
</div>

<hr>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=500&t=n&d=Tx8GmIEjPmbwlY4cYgulWgIOUbn1KU7bZJ7dF_YSWkI'></script>
</body>
</html>
